# -*- coding: utf-8 -*-
"""scrap-request-beaytifulsoup-done.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YsIm8ByFLnqAdHhCMsLxHe0NnD3vKlpP
"""

# !pip install requests
# !pip install beautifulsoup4

import random
import re
import sys
import time
import csv
import bs4
import numpy as np
import pandas as pd

import requests
from bs4 import BeautifulSoup

# URL of the website to scrape
url = "https://www.malkelapagading.com/directory"

# Send an HTTP GET request and fetch the HTML content
response = requests.get(url)
html_content = response.content
print(response.content)

# Create a BeautifulSoup object to parse the HTML content
soup = BeautifulSoup(html_content, "html.parser")
# print(soup.prettify())

param1 = soup.find_all('div', {'class':'work-process-text'})
param2 = soup.find_all('div', {'class':'work-process-details position-absolute display-block'})

data = []

for text, detail in zip(param1, param2):
  tenant = text.find('h5',{'class':'margin-two text-center'}).get_text(strip=True)
  tenant_url = detail.find('a',{'class': 'highlight-button-dark btn btn-very-small margin-three no-margin'}).get("href")

  r = requests.get(tenant_url)
  more = BeautifulSoup(r.content, "html.parser")

  tenant_floor = detail.find('p',{'class':'text-uppercase no-margin-bottom'}).get_text(strip=True)
  tenant_subfloor = detail.find('h6',{'no-margin-top margin-two-bottom'}).get_text(strip=True)

  ul_element = more.find("div", class_="breadcrumb").ul
  if ul_element:
    # Find all list items within the unordered list
    list_items = ul_element.find_all("li", class_="white-text")

    # Extract the text from the second and third list items
    if len(list_items) >= 2:
        category = list_items[0].get_text(strip=True)
        sub_category = list_items[1].get_text(strip=True)

  data.append([tenant, tenant_url, tenant_floor, tenant_subfloor, category, sub_category])

csv_filename = "tenant_data.csv"

with open(csv_filename, mode="w", newline="", encoding="utf-8") as csv_file:
    csv_writer = csv.writer(csv_file)

    # Write header
    csv_writer.writerow(["Tenant", "URL", "Floor", "Subfloor", "Category", "Subcategory"])

    # Write data
    csv_writer.writerows(data)